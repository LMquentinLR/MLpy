{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML.py, MLpy, MLPy!\n",
    "<p style=\"text-align: center;\">MLpy came as a simple question <i>and</i> a big project in pespective:\n",
    "<br><b>\"Can I build a discord bot that can tell two pictures apart?\"</b></p>\n",
    "<br>The goal of this notebook is two-fold with one overarching thread:\n",
    "\n",
    "1. To build a web crawler that can lift a statistically relevant number of images from [derpibooru](https://derpibooru.org), an image database powered by the community that built around the fourth generation of the show 'My Little Pony.'\n",
    "2. To build a machine learning algorithm capable of telling the difference between 2 types of pictures--to be summarized in a function that I can feed to my existing discord bot [BotJack](https://github.com/LMquentinLR/botjack_discord_bot).\n",
    "\n",
    "The thread is that I am, at the time of writing, learning how to program. I neither know how to build a web crawler or how a ML algorithm works (is it even called an algorithm?). All in all, this is a small idea that is both a learning experience, a blog--and of course a fun project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why a bot should do that?\n",
    "There are many reasons why a bot should be able to identify images posted on a server: classification, tagging, games, etc. \n",
    "<p style=\"text-align: center;\"><br>This notebook will focus on <b>compliance</b>.</p> \n",
    "\n",
    "* Servers may have anti-NSFW (i.e. not safe for watch) rules where explicit, grim, and otherwise unwanted content is banned or curtailed to specific server channels.\n",
    "* Moderation being volunteer-driven on discord, malicious users may capitalize on idle, asleep, or away-from-keyboard moderators to engage in rule-breaking activities. More commonly, users may simply post a NSFW picture in a SFW-only channel. \n",
    "* A bot able to distinguish NSFW content from SFW helps fill in the breaches that may affect any moderation effort. A bot, for instance, could automatically alert moderators when a specific content is posted and start a moderating process prior to any human intervention.\n",
    "\n",
    "<b>Automatic content moderation and compliance is a current industry effort in social media (e.g. Facebook)</b>, making this notebook a real world application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a web crawler\n",
    "Derpibooru is a website dedicated to fanart of MLP:FiM. It provides a JSON REST API for major site functionality, which can be freely used by anyone wanting to produce tools for the site or other webapps that use the data provided within Derpibooru.\n",
    "<br><b>Derpibooru licensing rules</b>\n",
    "<br>\"<i>Anyone can use it, users making abusively high numbers of requests may be asked to stop. Your application MUST properly cache, respect server-side cache expiry times. Your client MUST gracefully back off if requests fail (eg non-200 HTTP code), preferably exponentially or fatally.</i>\"\n",
    "\n",
    "<br>A single image can be accessed through the following links:\n",
    "1. https://derpibooru.org/2072316 (embedded)\n",
    "2. https://derpicdn.net/img/view/2019/6/22/2072316.png (default size)\n",
    "3. https://derpicdn.net/img/view/2019/6/22/2072316_small.png (small size)\n",
    "4. https://derpicdn.net/img/view/2019/6/22/2072316_medium.png (medium size)\n",
    "5. https://derpicdn.net/img/view/2019/6/22/2072316_large.png (large size)\n",
    "\n",
    "The metadata of a single picture can be accessed through the following link:\n",
    "* https://derpibooru.org/2072316.json\n",
    "<br> The list of attributes a single image is:\n",
    ">id, created_at, updated_at, first_seen_at, score, comment_count, width, height, file_name, description, uploader, uploader_id, image, upvotes, downvotes, faves, tags, tag_ids, aspect_ratio, original_format, mime_type, sha512_hash, orig_sha512_hash, source_url, representations, is_rendered, is_optimized, interactions, spoilered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class img_metadata:\n",
    "    \"\"\"\n",
    "    Class object corresponding to the process retrieving picture metadata from the REST API\n",
    "    of the website derpibooru--data is retrieved as a series of c. 1Mb JSON files.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tags = \"\", tags_include = True, instances = 50):\n",
    "        \"\"\"\n",
    "        Initialization of the img_metadata class object.\n",
    "        ---\n",
    "        :param <self>:         <class>    ; class object reference\n",
    "        :param <tags>:         <list>     ; list of strings (i.e. picture tags) used for sorting\n",
    "        :param <tags_include>: <boolean>  ; includes or excludes based on <tags>\n",
    "        :param <instances>:    <integer>  ; number of instances/loops allowed before program stops\n",
    "        \"\"\"\n",
    "        self.tags = tags\n",
    "        self.tags_include = tags_include\n",
    "        self.instances = instances\n",
    "    \n",
    "    def convert_bytes(self, bytes_size):\n",
    "        \"\"\"\n",
    "        Converts byte lengths\n",
    "        ---\n",
    "        :param <self>:       <class>   ; class object reference\n",
    "        :param <bytes_size>: <integer> ; size in bytes of a file\n",
    "        \"\"\"\n",
    "        for unit_multiple in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "            if bytes_size < 1024.0:\n",
    "                return \"%3.1f %s\" % (bytes_size, unit_multiple)\n",
    "            bytes_size /= 1024.0\n",
    "    \n",
    "    def keys_to_keep(self):\n",
    "        \"\"\"\n",
    "        Returns the keys to keep in the JSON extract\n",
    "        ---\n",
    "        :param <self>: <class> ; class object reference\n",
    "        \"\"\"\n",
    "        keys = [\"id\", \"created_at\", \"updated_at\", \"score\", \"uploader\",\n",
    "                \"uploader_id\", \"upvotes\", \"downvotes\", \"faves\", \"tags\",\n",
    "                \"tags_id\", \"representations\"]\n",
    "        return keys\n",
    "    \n",
    "    def json_split(self, path):\n",
    "        \"\"\"\n",
    "        Splits a json file if it is too large (1Mb)\n",
    "        ---\n",
    "        :param <self>: <class>  ; class object reference\n",
    "        :param <path>: <string> ; path of a json file\n",
    "        \"\"\"\n",
    "        length = self.convert_bytes(float(os.stat(path).st_size))\n",
    "        length = length.split(\" \")\n",
    "        if float(length[0]) >= 1.0 and length[1] == \"MB\":\n",
    "            nb_file = 0\n",
    "            while True:\n",
    "                new_path = path[:-5] + \"_\" + str(nb_file) + \".json\"\n",
    "                if os.path.exists(new_path) == False:\n",
    "                    print(\"SPLIT: JSON file to be split as 1Mb max size reached.\")\n",
    "                    os.rename(path, new_path) \n",
    "                    break\n",
    "                nb_file += 1\n",
    "    \n",
    "    def check_prior_extract(self, print_msg = True):\n",
    "        \"\"\"\n",
    "        Checks for existing metadata extractions in the working directory. \n",
    "        The default file name is 'derpibooru_metadata.json'.\n",
    "        ---\n",
    "        :param <self>:      <class>   ; class object reference\n",
    "        :param <print_msg>: <boolean> ; toggle to print message to command line\n",
    "        \"\"\"\n",
    "        \n",
    "        json_found = f\"FOUND: 'derpibooru_metadata.json'\"\n",
    "        json_not_found = f\"MISSING FILE: 'derpibooru_metadata.json'; NOT IN: {os.getcwd()}\\n\" + \\\n",
    "        \"FILE TO CREATE: 'derpibooru_metadata.json'\"\n",
    "        json_created = f\"FILE CREATED: 'derpibooru_metadata.json'\"\n",
    "        json_not_created = f\"ERROR FILE CREATION: 'derpibooru_metadata.json'\"\n",
    "        json_path = os.getcwd() + \"\\\\derpibooru_metadata.json\"\n",
    "        \n",
    "        #checks if a local file containing potential metadata exists\n",
    "        find = os.path.exists(json_path)\n",
    "        \n",
    "        #if TRUE: opens the file and extracts the contained metadata\n",
    "        #if FALSE: creates file storing an empty list\n",
    "        if find and print_msg: print(json_found)\n",
    "        elif not find and print_msg:\n",
    "            print(json_not_found)\n",
    "            try:    \n",
    "                with open(json_path, \"w\") as file: file.write(\"[]\")   \n",
    "                print(json_created) \n",
    "            except Exception as e: print(json_not_created, e, sep = \"\\n\")\n",
    "        elif not find and not print_msg:\n",
    "            try:    \n",
    "                with open(json_path, \"w\") as file: file.write(\"[]\")   \n",
    "            except Exception as e: print(json_not_created, e, sep = \"\\n\")\n",
    "        \n",
    "        return json_path\n",
    "\n",
    "    def json_collect(self, json_local, json_derpibooru, json_path):\n",
    "        \"\"\"\n",
    "        Collects picture metadata extracted from derpibooru\n",
    "        ---\n",
    "        :param <self>:            <class>       ; class object reference\n",
    "        :param <json_local>:      <json_object> ; JSON data stored locally\n",
    "        :param <json_derpibooru>: <json_object> ; JSON data extracted from derpibooru\n",
    "        :param <json_path>:       <string>      ; path of local file where the data is stored\n",
    "        \"\"\"\n",
    "        stored_keys = self.keys_to_keep()\n",
    "        last_id = -1\n",
    "        \n",
    "        for image_data in json_derpibooru:\n",
    "            \n",
    "            temp = image_data.copy()\n",
    "            \n",
    "            for item in image_data: \n",
    "                if item not in stored_keys: del temp[item]\n",
    "            \n",
    "            last_id = max(image_data[\"id\"], last_id)\n",
    "            \n",
    "            json_local.append(temp)\n",
    "            json_local.sort(key=operator.itemgetter(\"id\"), reverse = True)\n",
    "\n",
    "        with open(json_path,'w') as file: json.dump(json_local, file)\n",
    "        \n",
    "        self.json_split(json_path)\n",
    "        \n",
    "        return last_id\n",
    "\n",
    "    def crawl_metadata(self):\n",
    "        \"\"\"\n",
    "        Retrieves from the derpibooru REST API a list of picture metadata.\n",
    "        ---\n",
    "        :param <self>: <class> ; class object reference\n",
    "        \"\"\"\n",
    "        #initializes local variables    \n",
    "        iterations = self.instances\n",
    "        back_off_counter = 1\n",
    "        max_instances_reached = f\"The set maximum number of images to request was reached at {self.instances}.\"\n",
    "        exit_condition_1 = \"The crawler scraped the derpibooru metadata. The program will now close.\"\n",
    "        \n",
    "        #retrieves most recent recorded picture id\n",
    "        if os.path.exists(\"derpibooru_metadata.json\"): json_path = \"derpibooru_metadata.json\"\n",
    "        else: json_path = self.check_prior_extract()\n",
    "        \n",
    "        with open(json_path, \"r\") as file: requested_id = json.load(file)\n",
    "        if requested_id == []: requested_id = 1\n",
    "        else: \n",
    "            requested_id = requested_id[0][\"id\"] + 1\n",
    "        \n",
    "        for iteration in range(iterations):\n",
    "            current_page = f\"You are requesting the derpibooru page starting with the id {requested_id}.\"\n",
    "            error_json_extraction = f\"The program couldn't extract the page and \" + \\\n",
    "                                    \"will now proceed to an exponential back off.\"\n",
    "            \n",
    "            json_path = self.check_prior_extract(False)\n",
    "            \n",
    "            with open(json_path,'r') as file: json_local = json.load(file)\n",
    "            \n",
    "            print(current_page)\n",
    "            \n",
    "            path_derpibooru = \"https://derpibooru.org/images.json?constraint=id&order=a&gt=\" + str(requested_id)\n",
    "            \n",
    "            try:\n",
    "                json_derpibooru = requests.get(path_derpibooru).json()[\"images\"]\n",
    "                if json_derpibooru == []: raise DatabaseFullyCrawled\n",
    "                \n",
    "                requested_id = self.json_collect(json_local, json_derpibooru, json_path)\n",
    "                \n",
    "                #time delay to respect the API's license\n",
    "                time.sleep(.250)\n",
    "            \n",
    "            except DatabaseFullyCrawled:\n",
    "                print(exit_condition_1)\n",
    "                break\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(error_json_extraction)\n",
    "                print(f\"The error was the following: {e}.\\n The program will back \" + \\\n",
    "                      f\"off for {2**back_off_counter} seconds.\")\n",
    "                back_off_counter += 1\n",
    "                time.sleep(2 ** back_off_counter)\n",
    "    \n",
    "    def id_filter(self, tags, tags_include):\n",
    "        \"\"\"\n",
    "        Retrieves the IDs of the locally stored metadata that fit specific tag parameters\n",
    "        ---\n",
    "        :param <self>:         <class>    ; class object reference\n",
    "        :param <tags>:         <list>     ; list of strings (i.e. picture tags) used for sorting\n",
    "        :param <tags_include>: <boolean>  ; includes or excludes based on <tags>\n",
    "        \"\"\"\n",
    "        metadata_files_list = list(filter(lambda file: file.startswith(\"derpibooru_metadata\"), os.listdir(os.getcwd())))\n",
    "        id_list = []\n",
    "        \n",
    "        for fname in metadata_files_list:\n",
    "            \n",
    "            with open(fname,\"r\") as file: json_local = json.load(file)\n",
    "            if json_local == []: break\n",
    "            \n",
    "            if tags_include == True: \n",
    "                json_kept = list(\n",
    "                    filter(\n",
    "                        lambda item: all(tag in item[\"tags\"].split(\", \") for tag in tags), \n",
    "                        json_local))\n",
    "            else: \n",
    "                json_kept = list(\n",
    "                    filter(\n",
    "                        lambda item: not any(tag in item[\"tags\"].split(\", \") for tag in tags), \n",
    "                        json_local))\n",
    "            \n",
    "            id_list += list(map(lambda item: item[\"id\"], json_kept))\n",
    "            \n",
    "        return id_list\n",
    "    \n",
    "    def id_filter_plus(self, tags, tags_include):\n",
    "        \"\"\"\n",
    "        Retrieves the IDs of the locally stored metadata that fit specific tag parameters\n",
    "        ---\n",
    "        :param <self>:         <class>    ; class object reference\n",
    "        :param <tags>:         <list>     ; list of strings (i.e. picture tags) used for sorting\n",
    "        :param <tags_include>: <boolean>  ; includes or excludes based on <tags>\n",
    "        \"\"\"\n",
    "        metadata_files_list = list(\n",
    "            filter(\n",
    "                lambda file: file.startswith(\"derpibooru_metadata\"), \n",
    "                os.listdir(os.getcwd())))\n",
    "        \n",
    "        id_list = []\n",
    "        \n",
    "        for fname in metadata_files_list:\n",
    "            \n",
    "            with open(fname,\"r\") as file: json_local = json.load(file)\n",
    "            if json_local == []: break\n",
    "            \n",
    "            tags_remove = list(\n",
    "                map(lambda item: item[1:], \n",
    "                    list(filter(lambda item: item.startswith(\"-\"), tags))))\n",
    "            \n",
    "            tags_keep = list(\n",
    "                map(lambda item: item[1:], \n",
    "                    list(filter(lambda item: item.startswith(\"+\"), tags))))\n",
    "            \n",
    "            def all_or_any(boolean):\n",
    "                \"\"\"\n",
    "                Returns the function all() if parameter TRUE, else returns function any()\n",
    "                ---\n",
    "                :param <boolean>: <boolean> ; boolean value\n",
    "                \"\"\"\n",
    "                if boolean == True:\n",
    "                    return all\n",
    "                else:\n",
    "                    return any\n",
    "            \n",
    "            fltr = all_or_any(tags_include)\n",
    "            \n",
    "            json_kept = list(\n",
    "                filter(\n",
    "                    lambda item: fltr(tag in item[\"tags\"].split(\", \") for tag in tags_keep) \n",
    "                    and not any(tag in item[\"tags\"].split(\", \") for tag in tags_remove), \n",
    "                    json_local))\n",
    "            \n",
    "            id_list += list(map(lambda item: item[\"id\"], json_kept))\n",
    "            \n",
    "        return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class derpibooru_search(img_metadata):\n",
    "    \"\"\"\n",
    "    Class representing a search object that can prompt the derpibooru REST API and\n",
    "    retrieve both picture metadata and the affiliated pictures.\n",
    "    \"\"\"\n",
    "    \n",
    "    def change_search(self, tags = \"\", tags_include = True, instances = 50):\n",
    "        \"\"\"\n",
    "        Changes the arguments of the created object derpibooru_search.\n",
    "        ---\n",
    "        :param <self>:         <class>    ; class object reference\n",
    "        :param <tags>:         <list>     ; list of strings (i.e. picture tags) used for sorting\n",
    "        :param <tags_include>: <boolean>  ; includes or excludes based on <tags>\n",
    "        :param <instances>:    <integer>  ; number of instances/loops allowed before program stops\n",
    "        \"\"\"\n",
    "        self.tags = tags\n",
    "        self.tags_include = tags_include\n",
    "        self.instances = instances\n",
    "\n",
    "    def crawl(self):\n",
    "        \"\"\"\n",
    "        Changes the arguments of the created object derpibooru_search.\n",
    "        ---\n",
    "        :param <self>: <class> ; class object reference\n",
    "        \"\"\"\n",
    "        print(\"----|Entering Derpibooru Data Crawler code|----\")\n",
    "        self.crawl_metadata()\n",
    "        print(\"---------------|Exiting Program|---------------\")\n",
    "    \n",
    "    def retrieve_ids(self, version):\n",
    "        \"\"\"\n",
    "        Retrieves the IDs of the locally stored metadata that fit specific tag parameters\n",
    "        ---\n",
    "        :param <self>: <class> ; class object reference\n",
    "        :param <version>: <boolean> ; selects the version used to retrieve IDs\n",
    "        \"\"\"\n",
    "        print(\"----|Retrieving IDs based on tag selection|----\")\n",
    "        if version == True:\n",
    "            id_list = self.id_filter(self.tags, self.tags_include)\n",
    "        else:\n",
    "            id_list = self.id_filter_plus(self.tags, self.tags_include)\n",
    "        print(\"----------------|IDs retrieved|----------------\")\n",
    "        return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Error(Exception):\n",
    "    \"\"\"Base class for other exceptions\"\"\"\n",
    "    pass\n",
    "\n",
    "class DatabaseFullyCrawled(Error):\n",
    "    \"\"\"Raised when the crawler reached the last pages of derpibooru\"\"\"\n",
    "    pass\n",
    "\n",
    "class NewContentCrawled(Error):\n",
    "    \"\"\"Raised when the input value is too large\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = derpibooru_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling Tests\n",
    "1. Test #1 would retrieve all existing available picture metadata but will stop at the 10th requested page\n",
    "2. Test #2 will retrieve all existing available picture metadata (can take up to 60h as at July 2019)\n",
    "3. Test #3 would retrieve all existing avalable picture metadata not locally stored already by will stop at the 10th requested page\n",
    "4. Test #4 would retrieve all existing available picture metadata not locally stored already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.__dict__\n",
    "obj.change_search(instances = 10)\n",
    "obj.crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.__dict__\n",
    "obj.change_search(instances = \"\")\n",
    "obj.crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving IDs based on tag selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1\n",
    "This version of the search function can only take a simple list of strings (e.g. \"safe\") which <b>must</b> match cases. The [tags_include] variable indicates:\n",
    "1. if TRUE: the retrieved IDs will <b>only</b> be the ones which tags <b>contain all</b> of the items listed in the variable [tags]\n",
    "2. if FALSE: the retrieved IDs will <b>only</b> be the ones which tags <b>does not contain any</b> of the items listed in the variable [tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_v1 = [\"applejack\"]\n",
    "tags_include_v1 = True\n",
    "obj.change_search(tags = tags_v1, tags_include = tags_include_v1)\n",
    "id_list = obj.retrieve_ids(True)\n",
    "\n",
    "print(id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### version 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of the search function takes a list of strings which must match the following format:\n",
    "<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"+tag\" <i>or</i> \"-tag\"\n",
    "<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>+</b> indicates keeping the ID, <b>-</b> indicates removing the ID.\n",
    "<br><br>The [tags_include] variable indicates:\n",
    "1. if TRUE: the retrieved IDs will <b>only</b> be the ones which tags <b>contain all</b> of the items listed in the variable [tags] with a \"+\" prefix\n",
    "2. if FALSE: the retrieved IDs will <b>only</b> be the ones which tags <b>contain at least one</b> of the items listed in the variable [tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_v2 = [\"+applejack\"]\n",
    "tags_include_v2 = True\n",
    "obj.change_search(tags = tags_v2, tags_include = tags_include_v2)\n",
    "id_list = obj.retrieve_ids(False)\n",
    "\n",
    "print(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST\n",
    "#metadata_files_list = list(filter(lambda file: file.startswith(\"derpibooru_metadata\"), os.listdir(os.getcwd())))\n",
    "#for fname in metadata_files_list:\n",
    "#    with open(fname,\"r\") as file: json_local = json.load(file) \n",
    "#    for item in json_local:\n",
    "#        if (\"suggestive\" in item[\"tags\"].split(\", \") and \"applejack\" in item[\"tags\"].split(\", \")) and \"pony\" not in item[\"tags\"].split(\", \"):\n",
    "#            print(item[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving pictures based on retrieved IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
