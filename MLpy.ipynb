{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML.py, MLpy, MLPy!\n",
    "<p style=\"text-align: center;\">MLpy came as a simple question <i>and</i> a big project in pespective:\n",
    "<br><b>\"Can I build a discord bot that can tell two pictures apart?\"</b></p>\n",
    "<br>The goal of this notebook is two-fold with one overarching thread:\n",
    "\n",
    "1. To build a web crawler that can lift a statistically relevant number of images from [derpibooru](https://derpibooru.org), an image database powered by the community that built around the fourth generation of the show 'My Little Pony.'\n",
    "2. To build a machine learning algorithm capable of telling the difference between 2 types of pictures--to be summarized in a function that I can feed to my existing discord bot [BotJack](https://github.com/LMquentinLR/botjack_discord_bot).\n",
    "\n",
    "The thread is that I am, at the time of writing, learning how to program. I neither know how to build a web crawler or how a ML algorithm works (is it even called an algorithm?). All in all, this is a small idea that is both a learning experience, a blog--and of course a fun project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why a bot should do that?\n",
    "There are many reasons why a bot should be able to identify images posted on a server: classification, tagging, games, etc. \n",
    "<p style=\"text-align: center;\"><br>This notebook will focus on <b>compliance</b>.</p> \n",
    "\n",
    "* Servers may have anti-NSFW (i.e. not safe for watch) rules where explicit, grim, and otherwise unwanted content is banned or curtailed to specific server channels.\n",
    "* Moderation being volunteer-driven on discord, malicious users may capitalize on idle, asleep, or away-from-keyboard moderators to engage in rule-breaking activities. More commonly, users may simply post a NSFW picture in a SFW-only channel. \n",
    "* A bot able to distinguish NSFW content from SFW helps fill in the breaches that may affect any moderation effort. A bot, for instance, could automatically alert moderators when a specific content is posted and start a moderating process prior to any human intervention.\n",
    "\n",
    "<b>Automatic content moderation and compliance is a current industry effort in social media (e.g. Facebook)</b>, making this notebook a real world application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a web crawler\n",
    "Derpibooru is a website dedicated to fanart of MLP:FiM. It provides a JSON REST API for major site functionality, which can be freely used by anyone wanting to produce tools for the site or other webapps that use the data provided within Derpibooru.\n",
    "<br><b>Derpibooru licensing rules</b>\n",
    "<br>\"<i>Anyone can use it, users making abusively high numbers of requests may be asked to stop. Your application MUST properly cache, respect server-side cache expiry times. Your client MUST gracefully back off if requests fail (eg non-200 HTTP code), preferably exponentially or fatally.</i>\"\n",
    "\n",
    "<br>A single image can be accessed through the following links:\n",
    "1. https://derpibooru.org/2072316 (embedded)\n",
    "2. https://derpicdn.net/img/view/2019/6/22/2072316.png (default size)\n",
    "3. https://derpicdn.net/img/view/2019/6/22/2072316_small.png (small size)\n",
    "4. https://derpicdn.net/img/view/2019/6/22/2072316_medium.png (medium size)\n",
    "5. https://derpicdn.net/img/view/2019/6/22/2072316_large.png (large size)\n",
    "\n",
    "The metadata of a single picture can be accessed through the following link:\n",
    "* https://derpibooru.org/2072316.json\n",
    "<br> The list of attributes a single image is:\n",
    ">id, created_at, updated_at, first_seen_at, score, comment_count, width, height, file_name, description, uploader, uploader_id, image, upvotes, downvotes, faves, tags, tag_ids, aspect_ratio, original_format, mime_type, sha512_hash, orig_sha512_hash, source_url, representations, is_rendered, is_optimized, interactions, spoilered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class img_metadata:\n",
    "    \"\"\"\n",
    "    Class object corresponding to the process retrieving picture metadata from the REST API\n",
    "    of the website derpibooru--data is retrieved as a series of c. 1Mb JSON files.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tags = \"\", tags_include = True, instances = 50, crawl_all = True):\n",
    "        \"\"\"\n",
    "        Initialization of the img_metadata class object.\n",
    "        ---\n",
    "        :param <self>:         <class>    ; class object reference\n",
    "        :param <tags>:         <list>     ; list of strings (i.e. picture tags) used for sorting\n",
    "        :param <tags_include>: <boolean>  ; includes or excludes based on <tags>\n",
    "        :param <instances>:    <integer>  ; number of instances/loops allowed before program stops\n",
    "        :param <crawl_all>:    <boolean>  ; True: crawls the whole derpibooru metadata/False: updates\n",
    "                                            the locally stored metadata\n",
    "        \"\"\"\n",
    "        self.tags = tags\n",
    "        self.tags_include = tags_include\n",
    "        self.instances = instances\n",
    "        self.crawl_all = crawl_all\n",
    "    \n",
    "    def convert_bytes(self, bytes_size):\n",
    "        \"\"\"\n",
    "        Converts byte lengths\n",
    "        ---\n",
    "        :param <self>:       <class>   ; class object reference\n",
    "        :param <bytes_size>: <integer> ; size in bytes of a file\n",
    "        \"\"\"\n",
    "        for unit_multiple in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "            if bytes_size < 1024.0:\n",
    "                return \"%3.1f %s\" % (bytes_size, unit_multiple)\n",
    "            bytes_size /= 1024.0\n",
    "    \n",
    "    def keys_to_keep(self):\n",
    "        \"\"\"\n",
    "        Returns the keys to keep in the JSON extract\n",
    "        ---\n",
    "        :param <self>: <class> ; class object reference\n",
    "        \"\"\"\n",
    "        keys = [\"id\", \"created_at\", \"updated_at\", \"score\", \"uploader\",\n",
    "                \"uploader_id\", \"upvotes\", \"downvotes\", \"faves\", \"tags\",\n",
    "                \"tags_id\", \"representations\"]\n",
    "        return keys\n",
    "    \n",
    "    def push_one_up(self):\n",
    "        \"\"\"\n",
    "        Updates all the existing JSON records (1Mb splits) by incrementing their name by 1.\n",
    "        ---\n",
    "        :param <self>: <class> ; class object reference\n",
    "        \"\"\"\n",
    "        def push(name, depth):\n",
    "            if os.path.exists(name[:19] + \"_\" + str(int(name[20:-5])+1) +\".json\"):\n",
    "                push(name[:19] + \"_\" + str(int(name[20:-5])+1) +\".json\", depth)\n",
    "                os.rename(name,name[:19] + \"_\" + str(int(name[20:-5])+1) +\".json\")\n",
    "            else: \n",
    "                os.rename(name,name[:19] + \"_\" + str(int(name[20:-5])+1) +\".json\")\n",
    "\n",
    "        if os.path.exists(\"derpibooru_metadata_0.json\"): \n",
    "            depth = 0\n",
    "            push(\"derpibooru_metadata_0.json\", depth)\n",
    "        if os.path.exists(\"derpibooru_metadata.json\"):\n",
    "            os.rename(\"derpibooru_metadata.json\", \"derpibooru_metadata_0.json\")\n",
    "    \n",
    "    def check_prior_extract(self, print_msg = True):\n",
    "        \"\"\"\n",
    "        Checks for existing metadata extractions in the working directory. \n",
    "        The default file name is 'derpibooru_metadata.json'.\n",
    "        ---\n",
    "        :param <self>:      <class>   ; class object reference\n",
    "        :param <print_msg>: <boolean> ; toggle to print message to command line\n",
    "        \"\"\"\n",
    "        \n",
    "        json_found = f\"FOUND: 'derpibooru_metadata.json'\"\n",
    "        json_not_found = f\"MISSING FILE: 'derpibooru_metadata.json'; NOT IN: {os.getcwd()}\\n\" + \\\n",
    "        \"FILE TO CREATE: 'derpibooru_metadata.json'\"\n",
    "        json_created = f\"FILE CREATED: 'derpibooru_metadata.json'\"\n",
    "        json_not_created = f\"ERROR FILE CREATION: 'derpibooru_metadata.json'\"\n",
    "        json_path = os.getcwd() + \"\\\\derpibooru_metadata.json\"\n",
    "        \n",
    "        #checks if a local file containing potential metadata exists\n",
    "        find = os.path.exists(json_path)\n",
    "        \n",
    "        #if TRUE: opens the file and extracts the contained metadata\n",
    "        #if FALSE: creates file storing an empty list\n",
    "        if find and print_msg: print(json_found)\n",
    "        elif not find and print_msg:\n",
    "            print(json_not_found)\n",
    "            try:    \n",
    "                with open(json_path, \"w\") as file: file.write(\"[]\")   \n",
    "                print(json_created) \n",
    "            except Exception as e: print(json_not_created, e, sep = \"\\n\")\n",
    "        elif not find and not print_msg:\n",
    "            try:    \n",
    "                with open(json_path, \"w\") as file: file.write(\"[]\")   \n",
    "            except Exception as e: print(json_not_created, e, sep = \"\\n\")\n",
    "        \n",
    "        return json_path\n",
    "\n",
    "    def json_collect(self, json_local, json_derpibooru, json_path, iterations):\n",
    "        \"\"\"\n",
    "        Collects picture metadata extracted from derpibooru\n",
    "        ---\n",
    "        :param <self>:            <class>       ; class object reference\n",
    "        :param <json_local>:      <json_object> ; JSON data stored locally\n",
    "        :param <json_derpibooru>: <json_object> ; JSON data extracted from derpibooru\n",
    "        :param <json_path>:       <string>      ; path of local file where the data is stored\n",
    "        :param <instances>:       <integer>     ; number of instances/loops allowed before program stops\n",
    "        \"\"\"\n",
    "        stored_keys = self.keys_to_keep()\n",
    "        \n",
    "        for image_data in json_derpibooru:\n",
    "            \n",
    "            temp = image_data.copy()\n",
    "            \n",
    "            for item in image_data: \n",
    "                if item not in stored_keys: del temp[item]\n",
    "\n",
    "            json_local.append(temp)\n",
    "            json_local.sort(key=operator.itemgetter(\"id\"), reverse = True)\n",
    "\n",
    "            #increments the number of performed iterations by one\n",
    "            #breaks out if the upper limit of instances is reached\n",
    "            if isinstance(self.instances, str) == False:\n",
    "                if iterations == self.instances - 1:\n",
    "                    iterations += 1\n",
    "                    break\n",
    "                else: iterations += 1\n",
    "\n",
    "        with open(json_path,'w') as file: json.dump(json_local, file)\n",
    "\n",
    "        #splits the json file if it is too large (1Mb)\n",
    "        length = self.convert_bytes(float(os.stat(json_path).st_size))\n",
    "        length = length.split(\" \")\n",
    "        if float(length[0]) >= 1.0 and length[1] == \"MB\":\n",
    "            nb_file = 0\n",
    "            while True:\n",
    "                json_new_path = json_path[:-5] + \"_\" + str(nb_file) + \".json\"\n",
    "                if os.path.exists(json_new_path) == False:\n",
    "                    print(\"SPLIT: JSON file to be split as 1Mb max size reached.\")\n",
    "                    os.rename(json_path, json_new_path) \n",
    "                    break\n",
    "                nb_file += 1\n",
    "        \n",
    "        return iterations\n",
    "\n",
    "    def json_update(self, json_derpibooru, json_path, iterations, last_id):\n",
    "        \"\"\"\n",
    "        Merges the metadata already stored locally with that extracted from derpibooru.\n",
    "        ---\n",
    "        :param <self>:            <class>       ; class object reference\n",
    "        :param <json_derpibooru>: <json_object> ; JSON data extracted from derpibooru\n",
    "        :param <json_path>:       <string>      ; path of local file where the data is stored\n",
    "        :param <instances>:       <integer>     ; number of instances/loops allowed before program stops\n",
    "        :param <last_id>:         <integer>     ; most recent ID stored locally\n",
    "        \"\"\"            \n",
    "        stored_keys = self.keys_to_keep()\n",
    "        \n",
    "        try:\n",
    "            with open(json_path, \"r\") as file: json_local = json.load(file)\n",
    "            \n",
    "            for image_data in json_derpibooru:\n",
    "                \n",
    "                temp = image_data.copy()\n",
    "                \n",
    "                for item in image_data: \n",
    "                    if item not in stored_keys: del temp[item]\n",
    "                \n",
    "                if len(json_local) == 0: json_local.append(temp)\n",
    "                else:\n",
    "                    for image_json in json_local:\n",
    "                        if temp[\"id\"] == last_id: raise NewContentCrawled\n",
    "                        else:\n",
    "                            json_local.append(temp)\n",
    "                            json_local.sort(key=operator.itemgetter('id'), reverse = True)\n",
    "                            break\n",
    "                \n",
    "                #increments the number of performed iterations by one\n",
    "                #breaks out if the upper limit of instances is reached\n",
    "                if isinstance(self.instances, str) == False:\n",
    "                    if iterations == self.instances - 1:\n",
    "                        iterations += 1\n",
    "                        break\n",
    "                    else: iterations += 1\n",
    "            \n",
    "            with open(json_path,'w') as file: json.dump(json_local, file)\n",
    "            \n",
    "            #splits the json file if it is too large (1Mb)\n",
    "            length = self.convert_bytes(float(os.stat(json_path).st_size))\n",
    "            length = length.split(\" \")\n",
    "            if float(length[0]) >= 1.0 and length[1] == \"MB\":\n",
    "                self.push_one_up()\n",
    "                json_new_path = \"derpibooru_metadata_0.json\"\n",
    "                if os.path.exists(json_new_path) == False:\n",
    "                    print(\"JSON file will be split for size management.\")\n",
    "                    os.rename(json_path, json_new_path) \n",
    "        \n",
    "        except NewContentCrawled: iterations = \"END\"\n",
    "        \n",
    "        return iterations\n",
    "\n",
    "    def crawl_metadata(self):\n",
    "        \"\"\"\n",
    "        Retrieves from the derpibooru REST API a list of picture metadata.\n",
    "        ---\n",
    "        :param <self>: <class> ; class object reference\n",
    "        \"\"\"\n",
    "        #initializes local variables\n",
    "        page = 1\n",
    "        iterations = 0\n",
    "        back_off_counter = 1\n",
    "        max_instances_reached = f\"The set maximum number of images to request was reached at {self.instances}.\"\n",
    "        exit_condition_1 = \"The crawler scraped the whole derpibooru metadata. The program will now close.\"\n",
    "        exit_condition_2 = \"The crawler scraped the new content on derpibooru. The program will now close.\"\n",
    "        \n",
    "        #removes all existing json files if crawl_all == True\n",
    "        for fname in os.listdir(os.getcwd()):\n",
    "            if self.crawl_all and fname.startswith(\"derpibooru_metadata\"): \n",
    "                os.remove(os.path.join(os.getcwd(), fname))\n",
    "        \n",
    "        #retrieves most recent recorded picture id\n",
    "        if os.path.exists(\"derpibooru_metadata_0.json\"): json_path = \"derpibooru_metadata_0.json\"\n",
    "        else: json_path = self.check_prior_extract()\n",
    "        \n",
    "        with open(json_path, \"r\") as file:\n",
    "            last_id = json.load(file)\n",
    "        \n",
    "        if last_id == []: last_id = -1\n",
    "        else: \n",
    "            last_id = last_id[0][\"id\"]\n",
    "            if not self.crawl_all: self.push_one_up()        \n",
    "        \n",
    "        while True:\n",
    "            current_page = f\"You are requesting the page {page} of the derpibooru website.\"\n",
    "            error_json_extraction = f\"The program couldn't extract the page {page} and \" + \\\n",
    "                                    \"will now proceed to an exponential back off.\"\n",
    "            \n",
    "            json_path = self.check_prior_extract(False)\n",
    "            \n",
    "            with open(json_path,'r') as file: json_local = json.load(file)\n",
    "            \n",
    "            print(current_page)\n",
    "            \n",
    "            path_derpibooru = \"https://derpibooru.org/images.json?page=\" + str(page)\n",
    "            \n",
    "            try:\n",
    "                json_derpibooru = requests.get(path_derpibooru).json()[\"images\"]\n",
    "                if json_derpibooru == []: raise DatabaseFullyCrawled\n",
    "                \n",
    "                if self.crawl_all:\n",
    "                    iterations = self.json_collect(json_local, json_derpibooru, json_path, iterations)\n",
    "                else:\n",
    "                    iterations = self.json_update(json_derpibooru, json_path, iterations, last_id)\n",
    "                \n",
    "                if iterations == \"END\": raise NewContentCrawled\n",
    "                \n",
    "                page += 1\n",
    "                \n",
    "                #time delay to respect the API's license\n",
    "                time.sleep(1)\n",
    "                \n",
    "                if not isinstance(self.instances, str) and iterations >= self.instances:\n",
    "                        print(max_instances_reached)\n",
    "                        break\n",
    "            \n",
    "            except DatabaseFullyCrawled:\n",
    "                print(exit_condition_1)\n",
    "                break\n",
    "            \n",
    "            except NewContentCrawled:\n",
    "                print(exit_condition_2)\n",
    "                break\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(error_json_extraction)\n",
    "                print(f\"The error was the following: {e}.\\n The program will back \" + \\\n",
    "                      f\"off for {2**back_off_counter} seconds.\")\n",
    "                back_off_counter += 1\n",
    "                time.sleep(2 ** back_off_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class derpibooru_search(img_metadata):\n",
    "    \"\"\"\n",
    "    Class representing a search object that can prompt the derpibooru REST API and\n",
    "    retrieve both picture metadata and the affiliated pictures.\n",
    "    \"\"\"\n",
    "    \n",
    "    def change_search(self, tags = \"\", tags_include = True, instances = 50, crawl_all = True):\n",
    "        \"\"\"\n",
    "        Changes the arguments of the created object derpibooru_search.\n",
    "        ---\n",
    "        :param <self>:         <class>    ; class object reference\n",
    "        :param <tags>:         <list>     ; list of strings (i.e. picture tags) used for sorting\n",
    "        :param <tags_include>: <boolean>  ; includes or excludes based on <tags>\n",
    "        :param <instances>:    <integer>  ; number of instances/loops allowed before program stops\n",
    "        \"\"\"\n",
    "        self.tags = tags\n",
    "        self.tags_include = tags_include\n",
    "        self.instances = instances\n",
    "        self.crawl_all = crawl_all\n",
    "\n",
    "    def crawl(self):\n",
    "        \"\"\"\n",
    "        Changes the arguments of the created object derpibooru_search.\n",
    "        ---\n",
    "        :param <self>: <class> ; class object reference\n",
    "        \"\"\"\n",
    "        print(\"----|Entering Derpibooru Data Crawler code|----\")\n",
    "        self.crawl_metadata()\n",
    "        print(\"---------------|Exiting Program|---------------\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Error(Exception):\n",
    "    \"\"\"Base class for other exceptions\"\"\"\n",
    "    pass\n",
    "\n",
    "class DatabaseFullyCrawled(Error):\n",
    "    \"\"\"Raised when the crawler reached the last pages of derpibooru\"\"\"\n",
    "    pass\n",
    "\n",
    "class NewContentCrawled(Error):\n",
    "    \"\"\"Raised when the input value is too large\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a series of tests you can try out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obj = derpibooru_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.__dict__\n",
    "obj.change_search(instances = 10, crawl_all = True)\n",
    "obj.crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.__dict__\n",
    "obj.change_search(instances = \"\", crawl_all = True)\n",
    "obj.crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obj.__dict__\n",
    "obj.change_search(instances = 10, crawl_all = False)\n",
    "obj.crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.__dict__\n",
    "obj.change_search(instances = \"\", crawl_all = False)\n",
    "obj.crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
