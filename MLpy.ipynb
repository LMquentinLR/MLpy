{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML.py, MLpy, MLPy!\n",
    "<p style=\"text-align: center;\">MLpy came as a simple question <i>and</i> a big project in pespective:\n",
    "<br><b>\"Can I build a discord bot that can tell two pictures apart?\"</b></p>\n",
    "<br>The goal of this notebook is two-fold with one overarching thread:\n",
    "\n",
    "1. To build a web crawler that can lift a statistically relevant number of images from [derpibooru](https://derpibooru.org), an image database powered by the community that built around the fourth generation of the show 'My Little Pony.'\n",
    "2. To build a machine learning algorithm capable of telling the difference between 2 types of pictures--to be summarized in a function that I can feed to my existing discord bot [BotJack](https://github.com/LMquentinLR/botjack_discord_bot).\n",
    "\n",
    "The thread is that I am, at the time of writing, learning how to program. I neither know how to build a web crawler or how a ML algorithm works (is it even called an algorithm?). All in all, this is a small idea that is both a learning experience, a blog--and of course a fun project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why a bot should do that?\n",
    "There are many reasons why a bot should be able to identify images posted on a server: classification, tagging, games, etc. \n",
    "<p style=\"text-align: center;\"><br>This notebook will focus on <b>compliance</b>.</p> \n",
    "\n",
    "* Servers may have anti-NSFW (i.e. not safe for watch) rules where explicit, grim, and otherwise unwanted content is banned or curtailed to specific server channels.\n",
    "* Moderation being volunteer-driven on discord, malicious users may capitalize on idle, asleep, or away-from-keyboard moderators to engage in rule-breaking activities. More commonly, users may simply post a NSFW picture in a SFW-only channel. \n",
    "* A bot able to distinguish NSFW content from SFW helps fill in the breaches that may affect any moderation effort. A bot, for instance, could automatically alert moderators when a specific content is posted and start a moderating process prior to any human intervention.\n",
    "\n",
    "<b>Automatic content moderation and compliance is a current industry effort in social media (e.g. Facebook)</b>, making this notebook a real world application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a web crawler\n",
    "Derpibooru is a website dedicated to fanart of MLP:FiM. It provides a JSON REST API for major site functionality, which can be freely used by anyone wanting to produce tools for the site or other webapps that use the data provided within Derpibooru.\n",
    "<br><b>Derpibooru licensing rules</b>\n",
    "<br>\"<i>Anyone can use it, users making abusively high numbers of requests may be asked to stop. Your application MUST properly cache, respect server-side cache expiry times. Your client MUST gracefully back off if requests fail (eg non-200 HTTP code), preferably exponentially or fatally.</i>\"\n",
    "\n",
    "<br>A single image can be accessed through the following links:\n",
    "1. https://derpibooru.org/2072316 (embedded)\n",
    "2. https://derpicdn.net/img/view/2019/6/22/2072316.png (default size)\n",
    "3. https://derpicdn.net/img/view/2019/6/22/2072316_small.png (small size)\n",
    "4. https://derpicdn.net/img/view/2019/6/22/2072316_medium.png (medium size)\n",
    "5. https://derpicdn.net/img/view/2019/6/22/2072316_large.png (large size)\n",
    "\n",
    "The metadata of a single picture can be accessed through the following link:\n",
    "* https://derpibooru.org/2072316.json\n",
    "<br> The list of attributes a single image is:\n",
    ">id, created_at, updated_at, first_seen_at, score, comment_count, width, height, file_name, description, uploader, uploader_id, image, upvotes, downvotes, faves, tags, tag_ids, aspect_ratio, original_format, mime_type, sha512_hash, orig_sha512_hash, source_url, representations, is_rendered, is_optimized, interactions, spoilered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class img_metadata:\n",
    "    \"\"\"\n",
    "    Class representing the JSON metadata file that can be retrieved from the REST API\n",
    "    of the website derpibooru.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tags = \"\", tags_include = True, instances = 50, crawl_all = True):\n",
    "        \"\"\"\n",
    "        Initialization of the img_metadata class of objects.\n",
    "        ---\n",
    "        :param <self>:         <class>    ; bound object/variable's reference\n",
    "        :param <tags>:         <list>     ; list of strings representing tags to filter \n",
    "                                            the extraction\n",
    "        :param <tags_include>: <boolean>  ; True includes/False excludes based on tags\n",
    "        :param <instances>:    <integer>  ; number of instances to extract before a stop\n",
    "        :param <crawl_all>:    <boolean>  ; True implies the program crawls the whole derpibooru\n",
    "                                            metadata database/False implies the program updates\n",
    "                                            the locally stored data with newly added data.\n",
    "        \"\"\"\n",
    "        self.tags = tags\n",
    "        self.tags_include = tags_include\n",
    "        self.instances = instances\n",
    "        self.crawl_all = crawl_all\n",
    "    \n",
    "    def check_file(self):\n",
    "        \"\"\"\n",
    "        Checks for an existing extraction of image metadata in the working directory. \n",
    "        This default file name is 'derpibooru_metadata.json'.\n",
    "        ---\n",
    "        :param <self>: <class> ; bound object/variable's reference\n",
    "        \"\"\"\n",
    "        #messages to be displayed in the commande line\n",
    "        json_found_msg = f\"The file 'derpibooru_metadata.json' was found and read.\"\n",
    "        json_not_found_msg = f\"The file 'derpibooru_metadata.json' is missing from {os.getcwd()}.\\n\" + \\\n",
    "        \"An empty JSON file named 'derpibooru_metadata.json' will be created.\"\n",
    "        json_created_msg = f\"An empty JSON file named 'derpibooru_metadata.json' was created.\"\n",
    "        json_not_created_msg = f\"The file 'derpibooru_metadata.json' could not be created.\"\n",
    "        json_metadata_path = os.getcwd() + \"\\\\derpibooru_metadata.json\"\n",
    "        \n",
    "        #checks if a local file containing potential metadata exists\n",
    "        find = os.path.exists(json_metadata_path)\n",
    "        \n",
    "        #if TRUE: opens the file and extracts the stored metadata\n",
    "        #if FALSE: creates an empty file and stores an empty list\n",
    "        if find == True:\n",
    "            print(json_found_msg)\n",
    "            with open(json_metadata_path,'r') as file:\n",
    "                json_metadata = json.load(file)\n",
    "        else:\n",
    "            print(json_not_found_msg)\n",
    "            json_metadata = \"[]\"\n",
    "            try:    \n",
    "                with open(json_metadata_path, \"w\") as file:\n",
    "                    file.write(json_metadata)   \n",
    "                print(json_created_msg) \n",
    "            except Exception as e:\n",
    "                print(json_not_created_msg)\n",
    "                print(e)\n",
    "        return json_metadata_path\n",
    "\n",
    "    def json_merge(self, json_local, json_derpibooru, path_json, iterations, crawl_all):\n",
    "        \"\"\"\n",
    "        Merges the metadata already stored locally with that extracted from derpibooru.\n",
    "        ---\n",
    "        :param <self>:            <class>       ; bound object/variable's reference\n",
    "        :param <json_local>:      <json_object> ; JSON data stored locally\n",
    "        :param <json_derpibooru>: <json_object> ; JSON data extracted from derpibooru\n",
    "        :param <path_json>:       <string>      ; path of local file where metadata is stored\n",
    "        :param <crawl_all>:       <boolean>     ; True implies the program crawls the whole derpibooru\n",
    "                                                  metadata database/False implies the program updates\n",
    "                                                  the locally stored data with newly added data.\n",
    "        \"\"\"\n",
    "        #keys not excluded from JSON metadata retrieved through the derpibooru REST API\n",
    "        retained_keys = [\"id\", \"created_at\", \"updated_at\", \"score\", \"uploader\", \n",
    "                \"uploader_id\", \"upvotes\", \"downvotes\", \"faves\", \"tags\",\n",
    "                \"tags_id\", \"representations\"]\n",
    "        \n",
    "        #for each image in the extracted JSON file:\n",
    "        #records each item and removes any key that's not in retained_keys\n",
    "        #appends each item to the JSON file (replacing a previous entry if it exists)\n",
    "        #counts the number of images iterated over to break when the max number of \n",
    "        #instances is reached.\n",
    "        try:\n",
    "            for image_derpibooru in json_derpibooru:\n",
    "                #creates a copy of the image metadata\n",
    "                temp = image_derpibooru.copy()\n",
    "                #removes the unwanted keys\n",
    "                for item in image_derpibooru:\n",
    "                    if item not in retained_keys: del temp[item]\n",
    "                #appends to the local JSON data, replacing if a previous entry exists\n",
    "                if len(json_local) == 0: json_local.append(temp)\n",
    "                else:\n",
    "                    for index, image_json in enumerate(json_local):\n",
    "                        #print(temp[\"id\"], image_json[\"id\"], index, sep= \"|\")\n",
    "                        if temp[\"id\"] == image_json[\"id\"]:\n",
    "                            if not crawl_all:\n",
    "                                json_local.append(temp)\n",
    "                                json_local.sort(key=operator.itemgetter('id'), reverse = True)\n",
    "                                raise NewContentCrawled\n",
    "                            else:\n",
    "                                del json_local[index]\n",
    "                                json_local.append(temp)\n",
    "                                json_local.sort(key=operator.itemgetter('id'), reverse = True)\n",
    "                                break\n",
    "                    else:\n",
    "                        json_local.append(temp)\n",
    "                        json_local.sort(key=operator.itemgetter('id'), reverse = True)\n",
    "                #increments the number of performed iterations by one\n",
    "                #breaks out if the maximum number of instances declared by the class is reached\n",
    "                if isinstance(self.instances, str) == False:\n",
    "                    print(iterations, self.instances)\n",
    "                    if iterations == self.instances - 1:\n",
    "                        iterations += 1\n",
    "                        break\n",
    "                    else: iterations +=1\n",
    "            #dumps the updated local JSON data in its file\n",
    "            with open(path_json,'w') as file:\n",
    "                json.dump(json_local, file)\n",
    "        except NewContentCrawled:\n",
    "            iterations = \"END\"\n",
    "        return iterations\n",
    "\n",
    "    def crawl_metadata(self):\n",
    "        \"\"\"\n",
    "        Retrieves from the derpibooru REST API the full list of picture metadata.\n",
    "        ---\n",
    "        :param <self>: <class> ; bound object/variable's reference\n",
    "        \"\"\"\n",
    "        #initializes local variables\n",
    "        page = 1\n",
    "        iterations = 0\n",
    "        back_off_counter = 1\n",
    "        instances_extracted = f\"The set maximum number of images to request was reached at {self.instances}.\"\n",
    "        exit_condition_1 = \"The crawler scraped the whole derpibooru metadata. The program will now close.\"\n",
    "        exit_condition_2 = \"The crawler scraped the new content on derpibooru. The program will now close.\"\n",
    "        exit = \"--Exiting program--\"\n",
    "        \n",
    "        #retrieves existing locally stored metadata\n",
    "        path_json = self.check_file()\n",
    "        with open(path_json,'r') as file: json_local = json.load(file)\n",
    "        \n",
    "        #iterates over an <x> pages until <self.instances> image metadata has been retrieved\n",
    "        while True:\n",
    "            #messages to be displayed in the commande line\n",
    "            current_page = f\"You are requesting the page {page} of the derpibooru website.\"\n",
    "            error_json_extraction = f\"The program couldn't extract the page {page} and \" + \\\n",
    "                                    \"will now proceed to an exponential back off.\"\n",
    "            #retrieves the data from the {page} page on Derpibooru\n",
    "            print(current_page)\n",
    "            path_derpibooru = \"https://derpibooru.org/images.json?page=\" + str(page)\n",
    "            #tries to overwrite/update the locally stored metadata\n",
    "            try:\n",
    "                json_derpibooru = requests.get(path_derpibooru).json()[\"images\"]\n",
    "                if json_derpibooru == \"{\\\"images\\\":[],\\\"interactions\\\":[]}\":\n",
    "                    raise DatabaseFullyCrawled\n",
    "                iterations = self.json_merge(json_local, json_derpibooru, path_json, iterations, self.crawl_all)\n",
    "                if iterations == \"END\":\n",
    "                    raise NewContentCrawled\n",
    "                page += 1\n",
    "                #time delay to respect the API's license\n",
    "                time.sleep(1)\n",
    "                if isinstance(self.instances, str) == False:\n",
    "                    if iterations >= self.instances:\n",
    "                        print(instances_extracted)\n",
    "                        break\n",
    "            except DatabaseFullyCrawled:\n",
    "                print(exit_condition_1)\n",
    "                break\n",
    "            except NewContentCrawled:\n",
    "                print(exit_condition_2)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(error_json_extraction)\n",
    "                print(f\"The error was the following: {e}.\\n The program will back \" + \\\n",
    "                      f\"off for {2**back_off_counter} seconds.\")\n",
    "                time.sleep(2 ** back_off_counter)\n",
    "        pass\n",
    "    \n",
    "    def metadata_filter(self, tags, tags_include):\n",
    "        \"\"\"\n",
    "        Provides the list of derpibooru picture IDs and sizes available for extraction.\n",
    "        ---\n",
    "        :param <self>:         <class>   ; bound object/variable's reference\n",
    "        :param <tags>:         <list>    ; list of strings representing tags to filter\n",
    "                                           the extraction\n",
    "        :param <tags_include>: <boolean> ; True includes/False excludes based on tags\n",
    "        \"\"\"\n",
    "        #return \"{id: size}\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class derpibooru_search(img_metadata):\n",
    "    \"\"\"\n",
    "    Class representing a search object that can prompt the derpibooru REST API and\n",
    "    retrieve both picture metadata and the affiliated pictures.\n",
    "    \"\"\"\n",
    "    \n",
    "    def change_search(self, tags = \"\", tags_include = True, instances = 50, crawl_all = True):\n",
    "        \"\"\"\n",
    "        Changes the arguments of the created object derpibooru_search.\n",
    "        ---\n",
    "        :param <self>:         <class>    ; bound object/variable's reference\n",
    "        :param <tags>:         <list>     ; list of strings representing tags to filter \n",
    "                                            the extraction\n",
    "        :param <tags_include>: <boolean>  ; True includes/False excludes based on tags\n",
    "        :param <instances>:    <integer>  ; number of instances to extract before a stop\n",
    "        \"\"\"\n",
    "        self.tags = tags\n",
    "        self.tags_include = tags_include\n",
    "        self.instances = instances\n",
    "    \n",
    "    def crawl(self):\n",
    "        \"\"\"\n",
    "        Changes the arguments of the created object derpibooru_search.\n",
    "        ---\n",
    "        :param <self>: <class> ; bound object/variable's reference\n",
    "        \"\"\"\n",
    "        print(\"---|Entering Derpibooru Data Crawler Program|---\")\n",
    "        self.crawl_metadata()\n",
    "        print(\"---------------|Exiting Program|---------------\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Error(Exception):\n",
    "    \"\"\"Base class for other exceptions\"\"\"\n",
    "    pass\n",
    "\n",
    "class DatabaseFullyCrawled(Error):\n",
    "    \"\"\"Raised when the crawler reached the last pages of derpibooru\"\"\"\n",
    "    pass\n",
    "\n",
    "class NewContentCrawled(Error):\n",
    "    \"\"\"Raised when the input value is too large\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obj = derpibooru_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.__dict__\n",
    "obj.change_search(instances = \"\", crawl_all = True)\n",
    "obj.crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obj.crawl.__doc__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
