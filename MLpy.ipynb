{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML.py, MLpy, MLPy!\n",
    "<p style=\"text-align: center;\">MLpy came as a simple question <i>and</i> a big project in pespective:\n",
    "<br><b>\"Can I build a discord bot that can tell two pictures apart?\"</b></p>\n",
    "<br>The goal of this notebook is two-fold with one overarching thread:\n",
    "\n",
    "1. To build a web crawler that can lift a statistically relevant number of images from [derpibooru](https://derpibooru.org), an image database powered by the community that built around the fourth generation of the show 'My Little Pony.'\n",
    "2. To build a machine learning algorithm capable of telling the difference between 2 types of pictures--to be summarized in a function that I can feed to my existing discord bot [BotJack](https://github.com/LMquentinLR/botjack_discord_bot).\n",
    "\n",
    "The thread is that I am, at the time of writing, learning how to program. I neither know how to build a web crawler or how a ML algorithm works (is it even called an algorithm?). All in all, this is a small idea that is both a learning experience, a blog--and of course a fun project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why a bot should do that?\n",
    "There are many reasons why a bot should be able to identify images posted on a server: classification, tagging, games, etc. \n",
    "<p style=\"text-align: center;\"><br>This notebook will focus on <b>compliance</b>.</p> \n",
    "\n",
    "* Servers may have anti-NSFW (i.e. not safe for watch) rules where explicit, grim, and otherwise unwanted content is banned or curtailed to specific server channels.\n",
    "* Moderation being volunteer-driven on discord, malicious users may capitalize on idle, asleep, or away-from-keyboard moderators to engage in rule-breaking activities. More commonly, users may simply post a NSFW picture in a SFW-only channel. \n",
    "* A bot able to distinguish NSFW content from SFW helps fill in the breaches that may affect any moderation effort. A bot, for instance, could automatically alert moderators when a specific content is posted and start a moderating process prior to any human intervention.\n",
    "\n",
    "<b>Automatic content moderation and compliance is a current industry effort in social media (e.g. Facebook)</b>, making this notebook a real world application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import imageio\n",
    "import json\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers. normalization import BatchNormalization\n",
    "import operator\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import random\n",
    "from random import shuffle\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a web crawler\n",
    "Derpibooru is a website dedicated to fanart of MLP:FiM. It provides a JSON REST API for major site functionality, which can be freely used by anyone wanting to produce tools for the site or other webapps that use the data provided within Derpibooru.\n",
    "<br><b>Derpibooru licensing rules</b>\n",
    "<br>\"<i>Anyone can use it, users making abusively high numbers of requests may be asked to stop. Your application MUST properly cache, respect server-side cache expiry times. Your client MUST gracefully back off if requests fail (eg non-200 HTTP code), preferably exponentially or fatally.</i>\"\n",
    "\n",
    "<br>A single image can be accessed through the following links:\n",
    "1. https://derpibooru.org/2072316 (embedded)\n",
    "2. https://derpicdn.net/img/view/2019/6/22/2072316.png (default size)\n",
    "3. https://derpicdn.net/img/view/2019/6/22/2072316_small.png (small size)\n",
    "4. https://derpicdn.net/img/view/2019/6/22/2072316_medium.png (medium size)\n",
    "5. https://derpicdn.net/img/view/2019/6/22/2072316_large.png (large size)\n",
    "\n",
    "The metadata of a single picture can be accessed through the following link:\n",
    "* https://derpibooru.org/2072316.json\n",
    "<br> The list of attributes a single image is:\n",
    ">id, created_at, updated_at, first_seen_at, score, comment_count, width, height, file_name, description, uploader, uploader_id, image, upvotes, downvotes, faves, tags, tag_ids, aspect_ratio, original_format, mime_type, sha512_hash, orig_sha512_hash, source_url, representations, is_rendered, is_optimized, interactions, spoilered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class img_metadata:\n",
    "    \"\"\"\n",
    "    Class object that corresponds to the process retrieving picture metadata from the REST API\n",
    "    of the website derpibooru--data is retrieved as a series of est. 1Mb JSON files.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tags = \"\", at_least_one = True, instances = 10, machine_learning = False):\n",
    "        \"\"\"\n",
    "        Initializes of the img_metadata class object.\n",
    "        ---\n",
    "        :param <self>:             <class>   ; class object reference\n",
    "        :param <tags>:             <list>    ; list of strings (i.e. picture tags)\n",
    "        :param <at_least_one>:     <boolean> ; toggles 'at least one tag' option instead of 'all tags'\n",
    "        :param <instances>:        <integer> ; number of instances/loops allowed before program stops\n",
    "        :param <machine_learning>: <boolean> ; toggles retrieving thumbnails instead of large pictures\n",
    "        \"\"\"\n",
    "        self.tags = tags\n",
    "        self.at_least_one = at_least_one\n",
    "        self.instances = instances\n",
    "        self.machine_learning = machine_learning\n",
    "    \n",
    "    def convert_bytes(self, bytes_size):\n",
    "        \"\"\"\n",
    "        Converts byte lengths.\n",
    "        ---\n",
    "        :param <self>:       <class>   ; class object reference\n",
    "        :param <bytes_size>: <integer> ; size in bytes of a file\n",
    "        \"\"\"\n",
    "        for unit_multiple in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "            if bytes_size < 1024.0:\n",
    "                return \"%3.1f %s\" % (bytes_size, unit_multiple)\n",
    "            bytes_size /= 1024.0\n",
    "    \n",
    "    def keys_to_keep(self):\n",
    "        \"\"\"\n",
    "        Returns the keys to keep in the JSON extract.\n",
    "        ---\n",
    "        :param <self>: <class> ; class object reference\n",
    "        \"\"\"\n",
    "        keys = [\"id\", \"created_at\", \"updated_at\", \"score\", \"uploader\",\n",
    "                \"uploader_id\", \"upvotes\", \"downvotes\", \"faves\", \"tags\",\n",
    "                \"tags_id\", \"aspect_ratio\", \"representations\"]\n",
    "        return keys\n",
    "    \n",
    "    def json_split(self, path):\n",
    "        \"\"\"\n",
    "        Splits a json file if it is too large (1Mb).\n",
    "        ---\n",
    "        :param <self>: <class>  ; class object reference\n",
    "        :param <path>: <string> ; path of a json file\n",
    "        \"\"\"\n",
    "        length = self.convert_bytes(float(os.stat(path).st_size))\n",
    "        length = length.split(\" \")\n",
    "        if float(length[0]) >= 1.0 and length[1] == \"MB\":\n",
    "            nb_file = 0\n",
    "            while True:\n",
    "                new_path = path[:-5] + \"_\" + str(nb_file) + \".json\"\n",
    "                if os.path.exists(new_path) == False:\n",
    "                    print(\"SPLIT: JSON file to be split as 1Mb max size reached.\")\n",
    "                    os.rename(path, new_path) \n",
    "                    break\n",
    "                nb_file += 1\n",
    "    \n",
    "    def check_prior_extract(self, print_msg = True):\n",
    "        \"\"\"\n",
    "        Checks existing metadata extractions in the working directory. \n",
    "        The default file name is 'derpibooru_metadata.json'.\n",
    "        ---\n",
    "        :param <self>:      <class>   ; class object reference\n",
    "        :param <print_msg>: <boolean> ; toggle between 'prints message to command line' and 'prints nothing'\n",
    "        \"\"\"\n",
    "        \n",
    "        json_found = \"FOUND: 'derpibooru_metadata.json'\"\n",
    "        json_not_found = \"MISSING FILE: 'derpibooru_metadata.json'; NOT IN: folder 'data'\\n\" + \\\n",
    "                         \"FILE TO CREATE: 'derpibooru_metadata.json'\"\n",
    "        json_created = \"FILE CREATED: 'derpibooru_metadata.json'\"\n",
    "        json_not_created = \"ERROR FILE CREATION: 'derpibooru_metadata.json'\"\n",
    "        json_path = os.getcwd() + \"\\\\data\\\\derpibooru_metadata.json\"\n",
    "        \n",
    "        find = os.path.exists(json_path)\n",
    "        \n",
    "        #if TRUE: opens file and extracts the contained metadata\n",
    "        #if FALSE: creates file storing an empty list\n",
    "        if find:\n",
    "            if print_msg: print(json_found)\n",
    "        else:\n",
    "            if print_msg: print(json_not_found)\n",
    "            try:    \n",
    "                if not os.path.exists(os.getcwd() + \"\\\\data\"): os.makedirs(os.getcwd() + \"\\\\data\")\n",
    "                with open(json_path, \"w\") as file: file.write(\"[]\")   \n",
    "                print(json_created) \n",
    "            except Exception as e: print(json_not_created, e, sep = \"\\n\")\n",
    "        \n",
    "        return json_path\n",
    "\n",
    "    def crawl_metadata(self):\n",
    "        \"\"\"\n",
    "        Retrieves from the derpibooru REST API a list of picture metadata.\n",
    "        ---\n",
    "        :param <self>: <class> ; class object reference\n",
    "        \"\"\"\n",
    "        #initializes local variables    \n",
    "        iterations = self.instances\n",
    "        back_off_counter = 1\n",
    "        max_instances_reached = \"The set maximum number of images to request was reached \" + \\\n",
    "                                f\"at {self.instances}.\"\n",
    "        exit_condition = \"The crawler scraped the derpibooru metadata. The program will \" + \\\n",
    "                         \"now close.\"\n",
    "        \n",
    "        #retrieves most recent recorded picture id\n",
    "        if os.path.exists(os.getcwd() + \"\\\\data\\\\derpibooru_metadata.json\"): \n",
    "            json_path = os.getcwd() + \"\\\\data\\\\derpibooru_metadata.json\"\n",
    "        else: \n",
    "            json_path = self.check_prior_extract()\n",
    "        \n",
    "        with open(json_path, \"r\") as file: requested_id = json.load(file)\n",
    "        \n",
    "        if requested_id == []: requested_id = 1\n",
    "        else: requested_id = requested_id[0][\"id\"] + 1\n",
    "        \n",
    "        while True:\n",
    "            requested_page = \"You are requesting the derpibooru page starting with the \" + \\\n",
    "                             f\"id {requested_id}.\"\n",
    "            error_json_extraction = \"The program couldn't extract the page and \" + \\\n",
    "                                    \"will now proceed to an exponential back off.\"\n",
    "            \n",
    "            #checks if previous JSON was not renamed due to the 1Mb splitting\n",
    "            json_path = self.check_prior_extract(False)\n",
    "            \n",
    "            with open(json_path,'r') as file: json_local = json.load(file)\n",
    "            \n",
    "            print(requested_page)\n",
    "            \n",
    "            path_derpibooru = \"https://derpibooru.org/images.json?constraint=id&order=a&gt=\" + \\\n",
    "                              str(requested_id)\n",
    "            \n",
    "            try:\n",
    "                if type(iterations) == int:\n",
    "                    if iterations > 1: iterations -= 1\n",
    "                    else: break\n",
    "                \n",
    "                json_derpibooru = requests.get(path_derpibooru).json()[\"images\"]\n",
    "                if json_derpibooru == []: raise DatabaseFullyCrawled\n",
    "                \n",
    "                requested_id = self.json_collect(json_local, json_derpibooru, json_path)\n",
    "                \n",
    "                #time delay to respect the API's license\n",
    "                time.sleep(.250)\n",
    "            \n",
    "            except DatabaseFullyCrawled:\n",
    "                print(exit_condition)\n",
    "                break\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(error_json_extraction)\n",
    "                print(f\"The error was the following: {e}.\\n The program will back \" + \\\n",
    "                      f\"off for {2**back_off_counter} seconds.\")\n",
    "                back_off_counter += 1\n",
    "                time.sleep(2 ** back_off_counter)\n",
    "\n",
    "    def json_collect(self, json_local, json_derpibooru, json_path):\n",
    "        \"\"\"\n",
    "        Collects picture metadata extracted from derpibooru.\n",
    "        ---\n",
    "        :param <self>:            <class>       ; class object reference\n",
    "        :param <json_local>:      <json_object> ; JSON data stored locally\n",
    "        :param <json_derpibooru>: <json_object> ; JSON data extracted from derpibooru\n",
    "        :param <json_path>:       <string>      ; path of local file where the data is stored\n",
    "        \"\"\"\n",
    "        stored_keys = self.keys_to_keep()\n",
    "        last_id = -1\n",
    "        \n",
    "        for image_data in json_derpibooru:\n",
    "            \n",
    "            temp = image_data.copy()\n",
    "            \n",
    "            for item in image_data: \n",
    "                if item not in stored_keys: del temp[item]\n",
    "            \n",
    "            last_id = max(image_data[\"id\"], last_id)\n",
    "            \n",
    "            json_local.append(temp)\n",
    "            json_local.sort(key=operator.itemgetter(\"id\"), reverse = True)\n",
    "\n",
    "        with open(json_path,'w') as file: json.dump(json_local, file)\n",
    "        \n",
    "        self.json_split(json_path)\n",
    "        \n",
    "        return last_id\n",
    "    \n",
    "    def id_filter(self, tags, at_least_one, machine_learning = False):\n",
    "        \"\"\"\n",
    "        Retrieves the IDs of the locally stored metadata that fit specific tag parameters\n",
    "        ---\n",
    "        :param <self>:             <class>   ; class object reference\n",
    "        :param <tags>:             <list>    ; list of strings (i.e. picture tags)\n",
    "        :param <at_least_one>:     <boolean> ; toggles 'at least one tag' option instead of 'all tags'\n",
    "        :param <machine_learning>: <boolean> ; toggles retrieving thumbnails instead of large pictures\n",
    "        \"\"\"\n",
    "        def any_or_all(boolean):\n",
    "            \"\"\"\n",
    "            Returns the function any() if argument <boolean> TRUE, else returns function all()\n",
    "            ---\n",
    "            :param <boolean>: <boolean> ; boolean value\n",
    "            \"\"\"\n",
    "            if boolean == True:\n",
    "                return any\n",
    "            else:\n",
    "                return all\n",
    "        \n",
    "        metadata_files_list = filter(lambda file: file.startswith(\"derpibooru_metadata\"), \n",
    "                                     os.listdir(os.getcwd() + \"\\\\data\"))\n",
    "        metadata_files_list = list(metadata_files_list)\n",
    "        \n",
    "        id_list = []\n",
    "        url_list = []\n",
    "        \n",
    "        for fname in metadata_files_list:\n",
    "            \n",
    "            with open(os.getcwd() + \"\\\\data\\\\\" + fname,\"r\") as file: json_local = json.load(file)\n",
    "            if json_local == []: break\n",
    "            \n",
    "            tags_keep = list(filter(lambda item: item.startswith(\"+\"), tags))\n",
    "            tags_keep = list(map(lambda item: item[1:], tags_keep))\n",
    "            tags_remove = list(filter(lambda item: item.startswith(\"-\"), tags))\n",
    "            tags_remove = list(map(lambda item: item[1:], tags_remove))\n",
    "            \n",
    "            fltr = any_or_all(at_least_one)\n",
    "            filter_keep = lambda item: fltr(tag in item[\"tags\"].split(\", \") for tag in tags_keep)\n",
    "            filter_remove = lambda item: not any(tag in item[\"tags\"].split(\", \") for tag in tags_remove)\n",
    "            json_kept = list(filter(filter_keep, json_local))\n",
    "            json_kept = list(filter(filter_remove, json_kept))\n",
    "            \n",
    "            filter_id = lambda item: item[\"id\"]\n",
    "            if not machine_learning:\n",
    "                filter_url = lambda item: item[\"representations\"][\"large\"][2:]\n",
    "            else:\n",
    "                filter_url = lambda item: item[\"representations\"][\"thumb_small\"][2:]\n",
    "            id_list += list(map(filter_id, json_kept))\n",
    "            url_list += list(map(filter_url, json_kept))\n",
    "            \n",
    "        return list(zip(id_list, url_list))\n",
    "    \n",
    "    def repair_tags(self):\n",
    "        \"\"\"\n",
    "        Checks if all retrieved IDs have an available list of tags.\n",
    "        ---\n",
    "        :param <self>: <class> ; class object reference\n",
    "        \"\"\"\n",
    "        metadata_files_list = filter(lambda file: file.startswith(\"derpibooru_metadata\"), \n",
    "                                     os.listdir(os.getcwd() + \"\\\\data\"))\n",
    "        metadata_files_list = list(metadata_files_list)\n",
    "        \n",
    "        for fname in metadata_files_list:\n",
    "            \n",
    "            with open(os.getcwd() + \"\\\\data\\\\\" + fname, \"r\") as file: \n",
    "                \n",
    "                json_local = json.load(file)\n",
    "                if json_local == []:break\n",
    "                \n",
    "                for index, item in enumerate(json_local):\n",
    "\n",
    "                    if item[\"tags\"] == None:\n",
    "                        json_id = item[\"id\"]\n",
    "                        path_derpibooru = \"https://derpibooru.org/\" + str(json_id) + \".json\"\n",
    "\n",
    "                        try:\n",
    "                            json_derpibooru = requests.get(path_derpibooru).json()\n",
    "                            time.sleep(.250)\n",
    "                            if json_derpibooru[\"tags\"] == None: raise AbsentTagList\n",
    "                            json_local[index][\"tags\"] = json_derpibooru[\"tags\"]\n",
    "                            print(f\"The tags of the picture {json_id} were updated.\")\n",
    "\n",
    "                        except AbsentTagList:\n",
    "                            print(f\"The url request for the picture {json_id} returned an empty list of tags.\")\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                \n",
    "            with open(os.getcwd() + \"\\\\data\\\\\" + fname,'w') as file: json.dump(json_local, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class derpibooru_search(img_metadata):\n",
    "    \"\"\"\n",
    "    Class object that corresponds to a search prompting the derpibooru REST API and\n",
    "    retrieve both picture metadata and the affiliated pictures.\n",
    "    \"\"\"\n",
    "    \n",
    "    def change_search(self, tags = \"\", at_least_one = True, instances = 10, machine_learning = False):\n",
    "        \"\"\"\n",
    "        Changes the arguments of the created object derpibooru_search.\n",
    "        ---\n",
    "        :param <self>:             <class>   ; class object reference\n",
    "        :param <tags>:             <list>    ; list of strings (i.e. picture tags)\n",
    "        :param <at_least_one>:     <boolean> ; toggles 'at least one tag' option instead of 'all tags'\n",
    "        :param <instances>:        <integer> ; number of instances/loops allowed before program stops\n",
    "        :param <machine_learning>: <boolean> ; toggles retrieving thumbnails instead of large pictures\n",
    "        \"\"\"\n",
    "        self.tags = tags\n",
    "        self.at_least_one = at_least_one\n",
    "        self.instances = instances\n",
    "        self.machine_learning = machine_learning\n",
    "\n",
    "    def crawl(self):\n",
    "        \"\"\"\n",
    "        Changes the arguments of the created object derpibooru_search.\n",
    "        ---\n",
    "        :param <self>: <class> ; class object reference\n",
    "        \"\"\"\n",
    "        print(\"----|Entering Derpibooru Data Crawler code|----\")\n",
    "        self.crawl_metadata()\n",
    "        print(\"---------------|Exiting Program|---------------\")\n",
    "    \n",
    "    def retrieve_ids(self):\n",
    "        \"\"\"\n",
    "        Retrieves the IDs of the locally stored metadata that fit specific tag parameters.\n",
    "        ---\n",
    "        :param <self>: <class> ; class object reference\n",
    "        \"\"\"\n",
    "        print(\"----|Retrieving IDs based on tag selection|----\")\n",
    "        id_list = self.id_filter(self.tags, self.at_least_one, self.machine_learning)\n",
    "        print(\"----------------|IDs retrieved|----------------\")\n",
    "        return id_list\n",
    "    \n",
    "    def repair(self):\n",
    "        \"\"\"\n",
    "        Repairs missing tags of the locally stored metadata.\n",
    "        ---\n",
    "        :param <self>: <class> ; class object reference\n",
    "        \"\"\"\n",
    "        print(\"----|Repairing missing tags in stored JSON|----\")\n",
    "        self.repair_tags()\n",
    "        print(\"----------------|Tags repaired|----------------\")\n",
    "\n",
    "    def request_imgs(self, tags, id_list, nb_of_requests = None):\n",
    "        \"\"\"\n",
    "        Retrieves a number of images from derpibooru.\n",
    "        ---\n",
    "        :param <self>:           <class>   ; class object reference\n",
    "        :param <tags>:           <list>    ; list of strings (tags used to search ids)\n",
    "        :param <id_list>:        <list>    ; list of strings (i.e. picture id + url)\n",
    "        :param <nb_of_requests>: <integer> ; number of images to request\n",
    "        \"\"\"\n",
    "        print(\"------|Requesting images from derpribooru|-----\")\n",
    "        \n",
    "        assert isinstance(tags, list) and isinstance(id_list, list) and isinstance(nb_of_requests, int)\n",
    "        \n",
    "        img_path = os.getcwd() + \"\\\\data\\\\\" + \"\".join(sorted(tags))\n",
    "        if not os.path.exists(img_path): os.mkdir(img_path)\n",
    "        \n",
    "        for index, item in enumerate(id_list):\n",
    "            \n",
    "            image_id = str(item[0])\n",
    "            path_derpibooru = item[1]\n",
    "            extension = \".\" + path_derpibooru.split(\".\")[-1]\n",
    "            \n",
    "            if index > nb_of_requests: \n",
    "                break\n",
    "            \n",
    "            picture_path = img_path + \"\\\\\" + image_id\n",
    "            if os.path.exists(picture_path + \".png\"): \n",
    "                continue\n",
    "            \n",
    "            if not item[1].endswith((\"png\")):\n",
    "                nb_of_requests += 1\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                picture_path += extension\n",
    "                request = requests.get(\"http://\" + path_derpibooru)\n",
    "                if request.status_code == 200:\n",
    "                    with open(picture_path, 'wb') as f:\n",
    "                        f.write(request.content)\n",
    "                \n",
    "                time.sleep(.5)\n",
    "                \n",
    "                print(f\"The picture {image_id} was downloaded.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            \n",
    "        print(\"---------------|Images retrieved|--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Error(Exception):\n",
    "    \"\"\"Base class for other exceptions\"\"\"\n",
    "    pass\n",
    "\n",
    "class DatabaseFullyCrawled(Error):\n",
    "    \"\"\"Raised when the crawler reached the last pages of derpibooru\"\"\"\n",
    "    pass\n",
    "\n",
    "class NewContentCrawled(Error):\n",
    "    \"\"\"Raised when the input value is too large\"\"\"\n",
    "    pass\n",
    "\n",
    "class AbsentTagList(Error):\n",
    "    \"\"\"Raised when the key value of the key 'tags' in a dictionary is 'None'\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = derpibooru_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling\n",
    "1. Test #1 would retrieve all existing available picture metadata but will stop at the 10th requested page\n",
    "2. Test #2 will retrieve all existing available picture metadata (can take up to 60h as at July 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.__dict__\n",
    "obj.change_search(instances = 10)\n",
    "obj.crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obj.__dict__\n",
    "obj.change_search(instances = \"\")\n",
    "obj.crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, retrieved metadata would be missing their tags. The following repairs them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.repair()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving IDs based on tag selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of the search function takes a list of strings which must match the following format:\n",
    "<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"+tag\" <i>or</i> \"-tag\"\n",
    "<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>+</b> indicates keeping the ID for search, <b>-</b> indicates removing the ID for search.\n",
    "<br><br>The [at_least_one] variable indicates:\n",
    "1. if TRUE: the retrieved IDs will <b>only</b> be the ones which tags <b>contain at least one</b> of the items listed in the variable [tags] with a \"+\" prefix\n",
    "2. if FALSE: the retrieved IDs will <b>only</b> be the ones which tags <b>contain all</b> of the items listed in the variable [tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tags = [\"+safe\"]\n",
    "search_include = False\n",
    "obj.change_search(tags = search_tags, at_least_one = search_include, machine_learning = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = obj.retrieve_ids()\n",
    "\n",
    "print(len(id_list))\n",
    "print(type(id_list))\n",
    "print(\"\\nFirst five items retrieved:\")\n",
    "print(id_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving pictures based on retrieved IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_requests = len(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "obj.request_imgs(search_tags, id_list, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA EXPLORATION USING NUMPY, MATPLOLIB, PANDAS, GGPLOT2, PLOTLY, BOKEH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do we want to plot?\n",
    "\n",
    "1. number of pictures posted across time that fit the tag \"Rarijack\"\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tags = [\"+rarijack\"]\n",
    "search_include = True\n",
    "obj.change_search(tags = search_tags, \n",
    "                  at_least_one = search_include, \n",
    "                  machine_learning = False)\n",
    "id_list = obj.retrieve_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MatplotLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ggplot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE 1 - SEPARATING SFW FROM NSFW PICTURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading pictures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to download a set of pictures which can be used to fit a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(tags):\n",
    "    \"\"\"\n",
    "    Downloads pictures based on a list of tag.\n",
    "    ---\n",
    "    :param <tags>: list ; list of tags to sort and download pictures\n",
    "    \"\"\"\n",
    "    obj = derpibooru_search()\n",
    "    obj.change_search(tags = tags, at_least_one = True, machine_learning = True)\n",
    "    id_list = obj.retrieve_ids()\n",
    "    obj.request_imgs(tags, id_list, 80000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tags = [[\"+safe\",\"-comic\", \"-meme\", \"-text\"], \n",
    "        [\"+explicit\", \"+grimdark\", \"-comic\", \"-meme\", \"-text\"]]\n",
    "\n",
    "for tag in tags:\n",
    "    download(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning extracted pictures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retrieved pictures are all sized differently and some are unusable. We need to resize them and clean the folders from the unreadable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean_pictures(targets):\n",
    "    \"\"\"\n",
    "    Cleans the pictures that will be used for training and validation.\n",
    "    ---\n",
    "    :param <targets>: list ; list of folders to clean\n",
    "    \"\"\"\n",
    "    for folder in target_folders:\n",
    "        img_content = os.listdir(folder)\n",
    "        to_delete = []\n",
    "        for img in img_content:\n",
    "            if img[-4:] != \"jpeg\":\n",
    "                try:\n",
    "                    image = Image.open(folder + \"/\" + img)\n",
    "                    if 0.7 < image.size[0]/image.size[1] < 1.4:\n",
    "                        image = image.resize((224, 224))\n",
    "                        image.save(folder + \"/\" + img)\n",
    "                    else:\n",
    "                        image.close()\n",
    "                        os.remove(folder + \"/\" + img)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    os.remove(folder + \"/\" + img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downloaded pictures have been <b>manually</b> copied in \"copy\" folders to avoid overwriting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_folders = [\"./data/+safe-copy\", \"./data/+unsafe-copy\"]\n",
    "clean_pictures(target_folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on all the extracted pictures--and to make the process run in a timely manner on my Surface Pro 2--we are sampling the extracted data:\n",
    "\n",
    "- 1000 safe and unsafe pictures for training\n",
    "- 100 safe and unsafe pictures for testing/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample(folders, sample_desc):\n",
    "    \"\"\"\n",
    "    Samples existing pictures.\n",
    "    ---\n",
    "    :param <folders>: list ; list of folders to sample\n",
    "    :param <sample_desc>: list ; list of folders where to store samples \n",
    "                                 + nb of pictures to extract\n",
    "    \"\"\"\n",
    "    for model in model_description:\n",
    "        target_folder = model[0]\n",
    "        try:\n",
    "            shutil.rmtree(target_folder)\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "        os.makedirs(target_folder)\n",
    "        \n",
    "        selection = []\n",
    "        \n",
    "        for folder in folders:\n",
    "            counter = 0\n",
    "            stored_imgs = os.listdir(folder)\n",
    "            \n",
    "            while counter != model[1]:\n",
    "                choice = random.choice(stored_imgs)\n",
    "                if choice not in selection:\n",
    "                    selection.append(choice)\n",
    "                    shutil.copy(folder + \"/\" + choice, target_folder)\n",
    "                    os.rename(target_folder + \"/\" + choice,\n",
    "                             target_folder + \"/\" + folder[7:].split(\"-\")[0]\n",
    "                             + \"-\" + choice)\n",
    "                    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"./data/+safe-copy\", \"./data/+unsafe-copy\"]\n",
    "#model_description = [(\"./data/Training\", 500), \n",
    "#                     (\"./data/Validation\", 100), \n",
    "#                     (\"./data/Test\", 200)]\n",
    "model_description = [(\"./data/Training\", 1000),\n",
    "                     (\"./data/Test\", 100)]\n",
    "\n",
    "create_sample(folders, model_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is a rewrite of the following github: [here](https://github.com/CShorten/KaggleDogBreedChallenge/blob/master/DogBreed_BinaryClassification.ipynb?source=post_page---------------------------).\n",
    "Thanks to CShorten!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_img(name):\n",
    "    \"\"\"\n",
    "    Returns whether an image is safe or unsafe based on its name.\n",
    "    ---\n",
    "    :param <name>: string ; path/name of a picture\n",
    "    \"\"\"\n",
    "    word_label = name.split('-')[0]\n",
    "    if word_label.startswith(\"+safe\"): return np.array([1, 0])\n",
    "    else: return np.array([0, 1])\n",
    "\n",
    "def load_data(folder):\n",
    "    \"\"\"\n",
    "    Loads the safe/unsafe value of each picture stored in a specific folder.\n",
    "    ---\n",
    "    :param <filder>: string ; path of a folder\n",
    "    \"\"\"\n",
    "    test_data = []\n",
    "    for img in os.listdir(folder):\n",
    "        label = label_img(img)\n",
    "        path = os.path.join(folder, img)\n",
    "        img = Image.open(path)\n",
    "        img = img.convert('L')\n",
    "        img = img.resize((224, 224), Image.ANTIALIAS)\n",
    "        test_data.append([np.array(img), label])\n",
    "    shuffle(test_data)\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_description = [(\"./data/Training\", 1000),\n",
    "                     (\"./data/Test\", 100)]\n",
    "DIR_TRAINING = model_description[0][0]\n",
    "DIR_TESTING = model_description[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = load_data(DIR_TRAINING)\n",
    "plt.imshow(train_data[1][0], cmap = 'gist_gray')\n",
    "\n",
    "trainImages = np.array([i[0] for i in train_data]).reshape(-1, 224, 224, 1)\n",
    "trainLabels = np.array([i[1] for i in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_data(DIR_TESTING)    \n",
    "plt.imshow(test_data[1][0], cmap = 'gist_gray')\n",
    "\n",
    "testImages = np.array([i[0] for i in test_data]).reshape(-1, 224, 224, 1)\n",
    "testLabels = np.array([i[1] for i in test_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Modeling - TEST 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size = (3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(224, 224, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64,\n",
    "                 kernel_size=(3,3),\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(96,\n",
    "                 kernel_size=(3,3),\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(96,\n",
    "                 kernel_size=(3,3),\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64,\n",
    "                 kernel_size=(3,3),\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(2, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(trainImages, \n",
    "          trainLabels, \n",
    "          batch_size = 100, \n",
    "          epochs = 20,\n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(testImages, testLabels, verbose = 0)\n",
    "print(acc * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As at August 7th, the prediction power of the model over the test sample is of 71%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('NSFWdetector_20e100bs_v1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an already trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000 new safe and unsafe pictures have been sampled for training, along with 100 new safe and unsafe pictures for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('NSFWdetector_20e100bs_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"./data/+safe-copy\", \"./data/+unsafe-copy\"]\n",
    "#model_description = [(\"./data/Training\", 500), \n",
    "#                     (\"./data/Validation\", 100), \n",
    "#                     (\"./data/Test\", 200)]\n",
    "model_description = [(\"./data/Training\", 1000),\n",
    "                     (\"./data/Test\", 100)]\n",
    "\n",
    "create_sample(folders, model_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(trainImages, \n",
    "          trainLabels, \n",
    "          batch_size = 100, \n",
    "          epochs = 20,\n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(testImages, testLabels, verbose = 0)\n",
    "print(acc * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As at August 7th, training the first version of the model again worsens the prediction power of the model over a test sample by 100 basis points. <b>This is a potential case of overfitting</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('NSFWdetector_20e100bs_v2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Modeling - TEST 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size = (3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(224, 224, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(96,\n",
    "                 kernel_size=(3,3),\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(96,\n",
    "                 kernel_size=(3,3),\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64,\n",
    "                 kernel_size=(3,3),\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(2, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(trainImages, \n",
    "          trainLabels, \n",
    "          batch_size = 100, \n",
    "          epochs = 20,\n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(testImages, testLabels, verbose = 0)\n",
    "print(acc * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As at August 7th, the prediction power of the model over the test sample is of 71%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('NSFWdetector_20e100bs_v3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Modeling - TEST 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size = (3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(224, 224, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(96,\n",
    "                 kernel_size=(3,3),\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64,\n",
    "                 kernel_size=(3,3),\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(2, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(trainImages, \n",
    "          trainLabels, \n",
    "          batch_size = 100, \n",
    "          epochs = 20,\n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(testImages, testLabels, verbose = 0)\n",
    "print(acc * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As at August 7th, the prediction power of the model over the test sample is of 69.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('NSFWdetector_20e100bs_v4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Modeling - TEST 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size = (3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(224, 224, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(96,\n",
    "                 kernel_size=(3,3),\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(96,\n",
    "                 kernel_size=(3,3),\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64,\n",
    "                 kernel_size=(3,3),\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(2, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(trainImages, \n",
    "          trainLabels, \n",
    "          batch_size = 100, \n",
    "          epochs = 10,\n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(testImages, testLabels, verbose = 0)\n",
    "print(acc * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As at August 7th, the prediction power of the model over the test sample is of 65.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('NSFWdetector_20e100bs_v5.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Modeling - TEST 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size = (3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(224, 224, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(96,\n",
    "                 kernel_size=(3,3),\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64,\n",
    "                 kernel_size=(3,3),\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(2, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(trainImages, \n",
    "          trainLabels, \n",
    "          batch_size = 100, \n",
    "          epochs = 10,\n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(testImages, testLabels, verbose = 0)\n",
    "print(acc * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As at August 7th, the prediction power of the model over the test sample is of 54.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('NSFWdetector_20e100bs_v6.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results from Tests 1 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
